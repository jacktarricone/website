{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flush-career",
   "metadata": {},
   "source": [
    "# UAVSAR\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "*A 15 minute guide to UAVSAR data for SnowEX*\n",
    "- overview of UAVSAR data (both InSAR and PolSAR products)\n",
    "- demonstrate how to access and transform data\n",
    "- use Python rasterio and matlotlib\n",
    "```\n",
    "\n",
    "```{figure} ../../img/UAVSAR_plane.jpg\n",
    "---\n",
    "height: 400px\n",
    "name: UAVSAR\n",
    "---\n",
    "## Picture of UAVSAR. [Source](https://asf.alaska.edu/data-sets/sar-data-sets/uavsar/)\n",
    "```\n",
    "\n",
    "\n",
    "Intro slide deck: https://uavsar.jpl.nasa.gov/education/what-is-uavsar.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "from osgeo import gdal \n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import hvplot.xarray\n",
    "import pandas as pd # for DatetimeIndex \n",
    "import rioxarray\n",
    "import numpy as np #for log10, mean, percentile, power\n",
    "import rasterio as rio\n",
    "from rasterio.plot import show # plotting raster data\n",
    "from rasterio.plot import show_hist #histograms of raster data\n",
    "import codecs # for text parsing code\n",
    "import glob # for listing files in tiff conversion function\n",
    "import matplotlib.pyplot as plt # for add_subplot, axis, figure, imshow, legend, plot, set_axis_off, set_data,\n",
    "                                # set_title, set_xlabel, set_ylabel, set_ylim, subplots, title, twinx\n",
    "\n",
    "#from osgeo import gdal # for GetRasterBand, Open, ReadAsArray\n",
    "#import matplotlib\n",
    "#import matplotlib.pylab as plb # for add_patch, add_subplot, figure, hist, imshow, set_title, xaxis,_label, text \n",
    "#import matplotlib.patches as patches  # for Rectangle\n",
    "#import matplotlib.animation as an # for FuncAnimation\n",
    "#import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-lucas",
   "metadata": {},
   "source": [
    "## What is UAVSAR?\n",
    "\n",
    "UAVSAR stands for uninhabited aerial vehicle synthetic apeterature radar. It is a suborbital (airplane) remote sensing instrument operated out of NASA JPL.\n",
    "\n",
    "| frequency | resolution (rng x azi m) | swath width (km) |\n",
    "| - | - | - | \n",
    "| L-band (23 cm) | 1.8 | 16 | \n",
    "\n",
    "Documentation:\n",
    "* https://uavsar.jpl.nasa.gov/science/documents.html\n",
    "* https://asf.alaska.edu/data-sets/sar-data-sets/uavsar/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-viking",
   "metadata": {},
   "source": [
    "## NASA SnowEx 2020 and 2021 UAVSAR Campaings\n",
    "\n",
    "During the winter of 2020 and 2021, NASA conducted an L-band InSAR timeseris at a seris of sites across the Western US with the goal of tracking changes in SWE. Get site coordinate from HP to make map!!!!\n",
    "\n",
    "```{figure} ../../img/SnowEx2020.png\n",
    "---\n",
    "height: 400px\n",
    "name: UAVSAR map\n",
    "---\n",
    "##Map of the UAVSAR flight locations for NASA SnowEx. [Source](Chris Hiemstra)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-telephone",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "There are multiple ways to access UAVSAR data. Also the SQL database.\n",
    "\n",
    "* [Alaska Satellite Facility Vertex Portal](https://search.asf.alaska.edu/#/?dataset=UAVSAR)\n",
    "* [NASA Earthdata Suborbital Search](https://search.earthdata.nasa.gov/portal/suborbital/search?fi=UAVSAR&as[instrument][0]=UAVSAR)\n",
    "* [JPL UAVSAR Data Search](https://uavsar.jpl.nasa.gov/cgi-bin/data.pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-ready",
   "metadata": {},
   "source": [
    "https://www.reviewjournal.com/local/lake-mead-projected-to-match-lowest-water-level-in-history-this-week-2373770/## Data Types\n",
    "\n",
    "\n",
    "\n",
    "```{admonition} InSAR Data Types\n",
    ":class: InSAR Data Types\n",
    "- ANN file (.ann): a text annotation file with metadata\n",
    "- AMP files (.amp1 and .amp2): calibrated multi-looked amplitude products\n",
    "- INT files (.int): interferogram product, complex number format (we won't be using these here)\n",
    "- COR files (.cor): interferometric correlation product, a measure of the noise level of the phase\n",
    "- GRD files (.grd): interferometric products projected to the ground in simple geographic coordinates (latitude, longitude)\n",
    "- HGT file: the DEM that was used in the InSAR processing\n",
    "- KML and KMZ files (.kml or .kmz): format for viewing files in Google Earth (can't be used for analysis)\n",
    "```\n",
    "\n",
    "```{admonition} PolSAR Data Types\n",
    ":class: PolSAR Data Types\n",
    "-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory in which the notebook resides\n",
    "if 'tutorial_home_dir' not in globals():\n",
    "     tutorial_home_dir = os.getcwd()\n",
    "print(\"Notebook directory: \", tutorial_home_dir)\n",
    "\n",
    "if not os.path.exists('/tmp/'):\n",
    "    os.chdir('/tmp')\n",
    "   \n",
    "# directory for data downloads\n",
    "\n",
    "data_dir = os.path.join('/tmp')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this with YOUR NASA Earthdata login to download SLC data\n",
    "ASF_USER = input(\"Enter Username: \")\n",
    "ASF_PASS = getpass.getpass(\"Enter Password: \")\n",
    "\n",
    "files = ['https://datapool.asf.alaska.edu/AMPLITUDE_GRD/UA/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_amp_grd.zip',\n",
    "         'https://datapool.asf.alaska.edu/METADATA/UA/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_ann.zip',]\n",
    "         #'https://datapool.asf.alaska.edu/DEM_TIFF/UA/rockmt_32109_21017-013_21021-001_0012d_s01_L090_01_hgt_grd_tiff.zip']\n",
    "         \n",
    "if len(ASF_USER)==0 or len(ASF_PASS)==0:\n",
    "    raise Exception(\"Specifiy your ASF password and user (earthdata log-in)\")\n",
    "    \n",
    "for file in files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "        cmd = \"wget {0} --user={1} --password={2} -P {3} -nc\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "        print(f\"Downloading: {file}\")\n",
    "        os.system(cmd)\n",
    "    else:\n",
    "        print(filename + \" already exists. Skipping download ..\")\n",
    "print(\"done\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-investing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if downloaded\n",
    "\n",
    "print(glob.glob(\"/tmp/*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory='/tmp'\n",
    "#os.chdir(directory)\n",
    "#files=glob.glob('*.zip')\n",
    "\n",
    "#for f in files:\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip files just downloaded, this is broken and is looping over and filling up the memory\n",
    "\n",
    "# define file paths\n",
    "ann_zip = '/tmp/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_ann.zip'\n",
    "amp_zip = '/tmp/grmesa_27416_21019-017_21021-005_0006d_s01_L090_01_amp_grd.zip'\n",
    "#dem_zip = '/tmp/rockmt_32109_21017-013_21021-001_0012d_s01_L090_01_hgt_grd_tiff.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip\n",
    "\n",
    "# ann\n",
    "with zipfile.ZipFile(ann_zip, \"r\") as zip_ref:\n",
    "    zip_ref.printdir()\n",
    "    print('Extracting all the files now...')\n",
    "    zip_ref.extractall('/tmp')\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amp\n",
    "with zipfile.ZipFile(amp_zip, \"r\") as zip_ref2:\n",
    "    zip_ref2.printdir()\n",
    "    print('Extracting all the files now...')\n",
    "    zip_ref2.extractall('/tmp')\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dem \n",
    "#with zipfile.ZipFile(dem_zip, \"r\") as zip_ref3:\n",
    "#    zip_ref3.printdir()\n",
    "#    print('Extracting all the files now...')\n",
    "#    zip_ref3.extractall('/tmp')\n",
    "#    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up unwanted data from what we just downloaded\n",
    "\n",
    "directory='/tmp'\n",
    "os.chdir(directory)\n",
    "HV1_files=glob.glob('*HV_01.amp1.grd')\n",
    "VV1_files=glob.glob('*VV_01.amp1.grd')\n",
    "VH1_files=glob.glob('*VH_01.amp1.grd')\n",
    "\n",
    "for f in HV1_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VV1_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VH1_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amp2\n",
    "\n",
    "HV2_files=glob.glob('*HV_01.amp2.grd')\n",
    "VV2_files=glob.glob('*VV_01.amp2.grd')\n",
    "VH2_files=glob.glob('*VH_01.amp2.grd')\n",
    "\n",
    "for f in HV2_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VV2_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VH2_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hgt or dem\n",
    "\n",
    "hgt_HV_files=glob.glob('*HV_01.hgt.grd')\n",
    "hgt_VV_files=glob.glob('*VV_01.hgt.grd')\n",
    "hgt_VH_files=glob.glob('*VH_01.hgt.grd')\n",
    "\n",
    "for f in hgt_HV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in hgt_VV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in hgt_VH_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann\n",
    "ann_HV_files=glob.glob('*HV_01.ann')\n",
    "ann_VV_files=glob.glob('*VV_01.ann')\n",
    "ann_VH_files=glob.glob('*VH_01.ann')\n",
    "\n",
    "for f in ann_HV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in ann_VV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in ann_VH_files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glob.glob(\"/tmp/*.grd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-waterproof",
   "metadata": {},
   "source": [
    "## Converting Data to GeoTiffs\n",
    "\n",
    "The downloadable UAVSAR data comes in a flat binary format, which is not readable by GDAL. Therefore it needs to be transformed for use in standard spatial analysis software (ArcGIS, QGIS, Python, R, MATLAB, etc.) To do this, we will use the uavsar_tiff_convert function, which takes information (latitude, longitude, number of lines and samples, data type, pixel size) from the annotation file to create an ENVI header (.hdr). Once the ENVI header is created, the files can be read into Python and converted to GeoTiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(folder):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    os.chdir(folder)\n",
    "    int_file = glob.glob(os.path.join(folder, 'int.grd'))\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(folder):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(folder):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    print(Lines)\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    print(Samples)\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    print(Latitude)\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    print(Longitude)\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(folder):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(folder):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.grd')) # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs seperate save because of the complex format\n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.int.grd')) # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_CFloat32)\n",
    "\n",
    "    print(\".tiffs have been created\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-organization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zipped files\n",
    "#data_folder = \"/tmp\"\n",
    "#files_in_directory = os.listdir(data_folder)\n",
    "#zip_files = [file for file in files_in_directory if file.endswith(\".grd\")]\n",
    "#print(zip_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove zip files and check to see if work\n",
    "#for file in zip_files:\n",
    "#    path_to_file = os.path.join(data_folder, file)\n",
    "#    os.remove(path_to_file)\n",
    "#data = os.listdir(data_folder)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "uavsar_tiff_convert(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amplitude from the first aquisition\n",
    "for amp1 in glob.glob(\"*amp1.grd.tiff\"):\n",
    "    print(amp1)\n",
    "    \n",
    "# amplitude from the second aquisition\n",
    "for amp2 in glob.glob(\"*amp1.grd.tiff\"):\n",
    "    print(amp1)\n",
    "\n",
    "# coherence\n",
    "#for cor in glob.glob(\"*cor.grd.tiff\"):\n",
    " #   print(cor)\n",
    "\n",
    "# unwrapped phase\n",
    "#for unw in glob.glob(\"*unw.grd.tiff\"):\n",
    " #   print(unw)\n",
    "\n",
    "# dem used in processing\n",
    "for dem in glob.glob(\"*hgt.grd.tiff\"):\n",
    "    print(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "severe-navigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "amp1_rast = rio.open(amp1)\n",
    "amp2_rast = rio.open(amp2)\n",
    "#cor_rast = rio.open(cor)\n",
    "#unw_rast = rio.open(unw)\n",
    "dem_rast = rio.open(dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-shelter",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(amp1_rast)\n",
    "show(amp2_rast)\n",
    "#show(cor_rast)\n",
    "#show(unw_rast)\n",
    "show(dem_rast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-uganda",
   "metadata": {},
   "source": [
    "## Quick facts\n",
    "\n",
    "| frequency | resolution (rng x azi m) | swath width (km) |\n",
    "| - | - | - | \n",
    "| L-band | 1.8 | 16 | \n",
    "\n",
    "Documentation:\n",
    "* https://uavsar.jpl.nasa.gov/science/documents.html\n",
    "* https://asf.alaska.edu/data-sets/sar-data-sets/uavsar/\n",
    "\n",
    "Data Access:\n",
    "\n",
    "* [NASA Earthdata Suborbital Search](https://search.earthdata.nasa.gov/portal/suborbital/search?fi=UAVSAR&as[instrument][0]=UAVSAR)\n",
    "* [ASF Vertex SnowEx Grand Mesa Campaign]\n",
    "\n",
    "`https://search.asf.alaska.edu/#/?dataset=UAVSAR&mission=Grand%20Mesa,%20CO&resultsLoaded=true&granule=UA_grmesa_27416_21019-017_21021-005_0006d_s01_L090_01-AMPLITUDE_GRD&zoom=3&center=-92.747866,10.530273&productTypes=AMPLITUDE_GRD`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-perth",
   "metadata": {},
   "source": [
    "## API data access"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
